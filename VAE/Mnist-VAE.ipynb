{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_step(steps, num_bars):\n",
    "    assert steps > 1, \"Steps have to be > 1\"\n",
    "    import sys\n",
    "    import time\n",
    "    import datetime\n",
    "    idx = 0\n",
    "    start = time.time()\n",
    "    def step(info):\n",
    "        nonlocal steps\n",
    "        nonlocal num_bars\n",
    "        nonlocal idx\n",
    "        nonlocal start\n",
    "        \n",
    "        progress = int((idx/(steps-1))*100)\n",
    "        bars = int((idx/(steps-1))*num_bars)\n",
    "        elapsed = time.time() - start\n",
    "        avg_iter = elapsed/(idx+1)\n",
    "        remaining = (steps - idx)\n",
    "        eta = int(avg_iter * remaining)\n",
    "        sys.stdout.write('\\r')\n",
    "        # the exact output you're looking for:\n",
    "        sys.stdout.write(\"[{bars:{num_bars}}] {progress:3}% [ETA:{seconds}] {info}\".format(bars = '='*(bars-1)+'>', \n",
    "                                                                  progress = progress,\n",
    "                                                                  num_bars = num_bars,\n",
    "                                                                  seconds = datetime.timedelta(seconds = eta),\n",
    "                                                                  info = info))\n",
    "        sys.stdout.flush()\n",
    "        idx += 1\n",
    "    \n",
    "    return step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# steps = 100\n",
    "# step = get_step(steps,40)\n",
    "# for i in range(steps):\n",
    "#     step('Test')\n",
    "#     time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module): # pylint: disable=too-many-instance-attributes\n",
    "    \"\"\" VAE encoder \"\"\"\n",
    "    def __init__(self, img_channels, latent_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.latent_size = latent_size\n",
    "        self.img_channels = img_channels\n",
    "\n",
    "        self.conv1 = nn.Conv2d(img_channels, 32, 3, stride=2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3,stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, 3,stride=2)\n",
    "        \n",
    "        self.fc_mu = nn.Linear(2*2*64, latent_size)\n",
    "        self.fc_logsigma = nn.Linear(2*2*64, latent_size)\n",
    "\n",
    "\n",
    "    def forward(self, x): # pylint: disable=arguments-differ\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        mu = self.fc_mu(x)\n",
    "        logsigma = self.fc_logsigma(x)\n",
    "\n",
    "        return mu, logsigma\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\" VAE decoder \"\"\"\n",
    "    def __init__(self, img_channels, latent_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.latent_size = latent_size\n",
    "        self.img_channels = img_channels\n",
    "\n",
    "        self.fc1 = nn.Linear(latent_size, 256)\n",
    "        self.fc2 = nn.Linear(256, 784)\n",
    "        self.deconv = nn.ConvTranspose2d(32, img_channels, 3, stride=2)\n",
    "\n",
    "    def forward(self, x): # pylint: disable=arguments-differ\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # x = x.unsqueeze(-1).unsqueeze(-1)\n",
    "        reconstruction = F.sigmoid(self.fc2(x))\n",
    "        return reconstruction.view(-1,1,28,28)\n",
    "\n",
    "\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    \"\"\" Variational Autoencoder \"\"\"\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, x): # pylint: disable=arguments-differ\n",
    "        mu, logsigma = self.encoder(x)\n",
    "        \n",
    "        sigma = logsigma.exp()\n",
    "        eps = torch.randn_like(sigma)\n",
    "        z = eps.mul(sigma).add_(mu)\n",
    "        #z = torch.normal(mu,sigma) \n",
    "\n",
    "        recon_x = self.decoder(z)\n",
    "        \n",
    "        return recon_x, mu, logsigma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "def loss_function(recon_x, x, mu, logsigma):\n",
    "    \"\"\" VAE loss function \"\"\"\n",
    "    #BCE = F.mse_loss(recon_x, x, size_average=False)\n",
    "    BCE = F.binary_cross_entropy(recon_x, x, size_average=False)\n",
    "    \n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + 2 * logsigma - mu.pow(2) - (2 * logsigma).exp()) * 4\n",
    "    return BCE + KLD\n",
    "\n",
    "def train(epoch,epochs):\n",
    "    \"\"\" One training epoch \"\"\"\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    iterations = len(dataloader)\n",
    "    step = get_step(steps=iterations,num_bars=40)\n",
    "    for batch_idx, (data, _) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        step('Loss: {:.6f}'.format(loss.item() / len(data)))\n",
    "\n",
    "    print('\\nEpoch: {}/{} Average loss: {:.4f}'.format(\n",
    "        epoch+1,epochs,train_loss / len(dataloader.dataset)))\n",
    "\n",
    "\n",
    "def test():\n",
    "    \"\"\" One test epoch \"\"\"\n",
    "    model.eval()\n",
    "    dataset_test.load_next_buffer()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            data = data.to(device)\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            test_loss += loss_function(recon_batch, data, mu, logvar).item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))\n",
    "    return test_loss\n",
    "\n",
    "def get_one(generator):\n",
    "    return iter(generator).next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples:  60000\n"
     ]
    }
   ],
   "source": [
    "input_dim = 28 * 28\n",
    "batch_size = 128\n",
    "img_channels = 1\n",
    "latent_size = 10\n",
    "learning_rate = 1e-3\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor()])\n",
    "mnist = torchvision.datasets.MNIST('./', download=True, transform=transform)\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(mnist, batch_size=batch_size,\n",
    "                                         shuffle=True, num_workers=2)\n",
    "\n",
    "print('Number of samples: ', len(mnist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(img_channels, latent_size)\n",
    "decoder = Decoder(img_channels, latent_size)\n",
    "model = VAE(encoder,decoder)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=======================================>] 100% [ETA:0:00:00] Loss: 177.447713\n",
      "Epoch: 1/4 Average loss: 207.1918\n",
      "[=======================================>] 100% [ETA:0:00:00] Loss: 166.151988\n",
      "Epoch: 2/4 Average loss: 166.9801\n",
      "[=======================================>] 100% [ETA:0:00:00] Loss: 156.525675\n",
      "Epoch: 3/4 Average loss: 159.8736\n",
      "[=======================================>] 100% [ETA:0:00:00] Loss: 151.543254\n",
      "Epoch: 4/4 Average loss: 157.1898\n"
     ]
    }
   ],
   "source": [
    "iterations = 4\n",
    "for iteration in range(iterations):\n",
    "    train(iteration,iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1, 28, 28])\n",
      "torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "im = get_one(dataloader)\n",
    "print(im[0].shape)\n",
    "print(im[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x11e8491d0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test = im[0][0][0] #+ torch.Tensor(np.random.normal(0,0.3,[28,28]))\n",
    "plt.imshow(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x10f338e48>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "recon_x, mu, logsigma = model(test.unsqueeze(0).unsqueeze(0))\n",
    "print(recon_x.shape)\n",
    "plt.imshow(recon_x[0][0].detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, m1: [1600 x 2], m2: [10 x 256] at /Users/soumith/minicondabuild3/conda-bld/pytorch_1524590658547/work/aten/src/TH/generic/THTensorMath.c:2033",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-a44325c25b72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mt_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdigit_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdigit_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# decode for each square in the grid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-6e2bda405d55>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# pylint: disable=arguments-differ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;31m# x = x.unsqueeze(-1).unsqueeze(-1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mreconstruction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m    990\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 992\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    993\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    994\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, m1: [1600 x 2], m2: [10 x 256] at /Users/soumith/minicondabuild3/conda-bld/pytorch_1524590658547/work/aten/src/TH/generic/THTensorMath.c:2033"
     ]
    }
   ],
   "source": [
    "# Display a 2D manifold of the digits\n",
    "n = 40  # figure with 20x20 digits\n",
    "digit_size = 28\n",
    "figure = np.zeros((digit_size * n, digit_size * n))\n",
    "\n",
    "# Construct grid of latent variable values\n",
    "x = np.linspace(0.05, 0.95, n)\n",
    "y = np.linspace(0.05, 0.95, n)\n",
    "grid_x, grid_y = np.meshgrid(x,y)\n",
    "\n",
    "t_x = torch.Tensor(grid_x).view(-1,1)\n",
    "t_y = torch.Tensor(grid_y).view(-1,1)\n",
    "t = torch.cat((t_x,t_y),dim=1)\n",
    "samples = decoder(t).detach().numpy().reshape(n,n,digit_size,digit_size)\n",
    "\n",
    "# decode for each square in the grid\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        digit = samples[i][j]\n",
    "        figure[i * digit_size: (i + 1) * digit_size,\n",
    "               j * digit_size: (j + 1) * digit_size] = digit\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.imshow(figure, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_x = torch.Tensor(grid_x).view(-1,1)\n",
    "t_y = torch.Tensor(grid_y).view(-1,1)\n",
    "t = torch.cat((t_x,t_y),dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = decoder(t).detach().numpy().reshape(n,n,digit_size,digit_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, logsigma = encoder(im[0])\n",
    "sigma = logsigma.exp()\n",
    "eps = torch.randn_like(sigma)\n",
    "z = eps.mul(sigma).add_(mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 2])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = decoder(z[0].unsqu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x12b70d128>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(res[5][0].detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z[5].unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "optimizer = optim.Adam(vae.parameters(), lr=0.0001)\n",
    "l = None\n",
    "for epoch in range(100):\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        inputs, classes = data\n",
    "        inputs, classes = Variable(inputs.resize_(batch_size, input_dim)), Variable(classes)\n",
    "        optimizer.zero_grad()\n",
    "        dec = vae(inputs)\n",
    "        ll = latent_loss(vae.z_mean, vae.z_sigma)\n",
    "        loss = criterion(dec, inputs) + ll\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        l = loss.data[0]\n",
    "    print(epoch, l)\n",
    "\n",
    "plt.imshow(vae(inputs).data[0].numpy().reshape(28, 28), cmap='gray')\n",
    "plt.show(block=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 550.572632\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 309.673523\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 236.217880\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 225.817917\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 209.388962\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 206.767288\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 209.222809\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 200.530899\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 190.547012\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 190.907318\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 187.600830\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 172.074402\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 177.232468\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 171.121414\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 168.403152\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 162.337891\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 158.159317\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 158.383118\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 155.736801\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 157.516815\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 152.079147\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 151.069458\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 146.875183\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 147.536804\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 143.582718\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 142.833023\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 147.386459\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 148.989807\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 139.948090\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 142.725327\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 138.709061\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 137.972488\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 134.600403\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 137.896637\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 134.156281\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 134.010818\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 132.532883\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 136.062515\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 133.290726\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 132.390579\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 132.606857\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 131.448395\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 129.398636\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 132.150238\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 131.063339\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 127.112961\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 126.860794\n",
      "====> Epoch: 1 Average loss: 164.2587\n",
      "====> Test set loss: 119.4120\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 123.551399\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 122.690353\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 123.666115\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 125.134995\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 129.135941\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 126.217560\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 125.897552\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 120.687881\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 125.858208\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 129.608429\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 125.598251\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 124.201424\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 122.834839\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 126.512482\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 127.165565\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 123.705902\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 120.483101\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 119.318321\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 124.871597\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 118.418785\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 124.376335\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 121.277786\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 120.049942\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 117.597847\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 124.166557\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 120.010971\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 120.451279\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 119.016678\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 122.738091\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 120.355682\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 117.084839\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 122.067505\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 120.081070\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 116.244278\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 120.605423\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 120.371704\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 116.329765\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 118.000183\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 117.508110\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 119.602562\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 117.658424\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 118.964127\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 118.241516\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 119.622452\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 115.566666\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 117.023987\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 114.730148\n",
      "====> Epoch: 2 Average loss: 121.1655\n",
      "====> Test set loss: 107.1700\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 117.088654\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 114.987045\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 116.530304\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 115.368782\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 113.025505\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 118.567291\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 117.067017\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 115.664207\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 113.471825\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 112.075325\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 116.776764\n",
      "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 116.683426\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 112.620064\n",
      "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 113.248764\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 116.558083\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 116.949829\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 113.665123\n",
      "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 116.460228\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 117.868874\n",
      "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 116.375061\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 116.533531\n",
      "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 116.204704\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 115.034058\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 109.071564\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 112.368477\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 113.029762\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 116.368515\n",
      "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 109.676537\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 111.510239\n",
      "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 116.716064\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 113.062302\n",
      "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 113.398499\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 112.891228\n",
      "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 113.589172\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 111.711121\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 114.202347\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 113.809174\n",
      "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 113.596237\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 113.291275\n",
      "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 113.284851\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 108.514870\n",
      "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 110.756454\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 107.414566\n",
      "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 113.839577\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 114.360764\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 111.948044\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 111.566788\n",
      "====> Epoch: 3 Average loss: 114.2463\n",
      "====> Test set loss: 102.6085\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 116.696915\n",
      "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 114.183044\n",
      "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 112.963417\n",
      "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 110.910904\n",
      "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 113.108864\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 108.925667\n",
      "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 112.509415\n",
      "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 109.823662\n",
      "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 107.981247\n",
      "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 110.141861\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 114.222641\n",
      "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 112.140991\n",
      "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 115.501991\n",
      "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 111.728943\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 113.810966\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 110.596909\n",
      "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 112.290855\n",
      "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 114.098045\n",
      "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 112.974152\n",
      "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 113.832870\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 110.029922\n",
      "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 112.108093\n",
      "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 111.910477\n",
      "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 109.208374\n",
      "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 106.530945\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 110.315308\n",
      "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 110.747658\n",
      "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 104.247772\n",
      "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 109.955154\n",
      "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 112.263504\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 109.734283\n",
      "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 112.842484\n",
      "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 108.529259\n",
      "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 110.044090\n",
      "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 111.144691\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 107.092331\n",
      "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 107.346657\n",
      "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 112.320602\n",
      "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 107.266930\n",
      "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 108.420425\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 109.751877\n",
      "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 112.857292\n",
      "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 112.302773\n",
      "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 108.275223\n",
      "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 112.815933\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 107.031296\n",
      "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 109.823997\n",
      "====> Epoch: 4 Average loss: 111.2749\n",
      "====> Test set loss: 100.6654\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 111.966614\n",
      "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 112.899231\n",
      "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 108.723816\n",
      "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 106.516327\n",
      "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 108.486359\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 112.262070\n",
      "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 109.672997\n",
      "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 111.146172\n",
      "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 115.513474\n",
      "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 107.154892\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 109.261009\n",
      "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 110.929733\n",
      "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 116.989288\n",
      "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 112.888489\n",
      "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 114.684013\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 112.099915\n",
      "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 105.658676\n",
      "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 109.090668\n",
      "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 111.083771\n",
      "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 111.284653\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 112.933807\n",
      "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 110.111519\n",
      "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 111.337051\n",
      "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 110.232712\n",
      "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 112.183014\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 110.599068\n",
      "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 108.820145\n",
      "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 108.233681\n",
      "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 108.038246\n",
      "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 111.336655\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 105.459305\n",
      "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 117.687462\n",
      "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 111.482788\n",
      "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 107.515823\n",
      "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 109.990585\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 112.506065\n",
      "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 109.700653\n",
      "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 106.593323\n",
      "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 108.915482\n",
      "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 111.641045\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 107.914261\n",
      "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 111.008881\n",
      "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 110.820877\n",
      "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 109.195435\n",
      "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 112.692375\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 110.121399\n",
      "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 114.193893\n",
      "====> Epoch: 5 Average loss: 109.5038\n",
      "====> Test set loss: 99.4960\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 104.441681\n",
      "Train Epoch: 6 [1280/60000 (2%)]\tLoss: 108.957031\n",
      "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 108.354828\n",
      "Train Epoch: 6 [3840/60000 (6%)]\tLoss: 105.441292\n",
      "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 108.522079\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 104.384842\n",
      "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 110.382866\n",
      "Train Epoch: 6 [8960/60000 (15%)]\tLoss: 108.854950\n",
      "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 111.297913\n",
      "Train Epoch: 6 [11520/60000 (19%)]\tLoss: 110.268616\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 107.124237\n",
      "Train Epoch: 6 [14080/60000 (23%)]\tLoss: 109.968994\n",
      "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 111.528358\n",
      "Train Epoch: 6 [16640/60000 (28%)]\tLoss: 105.440987\n",
      "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 110.342316\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 108.539825\n",
      "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 109.445335\n",
      "Train Epoch: 6 [21760/60000 (36%)]\tLoss: 110.262329\n",
      "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 104.620094\n",
      "Train Epoch: 6 [24320/60000 (41%)]\tLoss: 110.147064\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 111.290916\n",
      "Train Epoch: 6 [26880/60000 (45%)]\tLoss: 102.736168\n",
      "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 108.199631\n",
      "Train Epoch: 6 [29440/60000 (49%)]\tLoss: 108.345253\n",
      "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 106.733459\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 109.106918\n",
      "Train Epoch: 6 [33280/60000 (55%)]\tLoss: 102.844498\n",
      "Train Epoch: 6 [34560/60000 (58%)]\tLoss: 105.714302\n",
      "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 109.933327\n",
      "Train Epoch: 6 [37120/60000 (62%)]\tLoss: 109.375183\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 103.370903\n",
      "Train Epoch: 6 [39680/60000 (66%)]\tLoss: 111.642738\n",
      "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 101.854706\n",
      "Train Epoch: 6 [42240/60000 (70%)]\tLoss: 110.906654\n",
      "Train Epoch: 6 [43520/60000 (72%)]\tLoss: 110.466087\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 108.928337\n",
      "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 105.841858\n",
      "Train Epoch: 6 [47360/60000 (79%)]\tLoss: 112.338577\n",
      "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 107.289276\n",
      "Train Epoch: 6 [49920/60000 (83%)]\tLoss: 105.709854\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 106.663635\n",
      "Train Epoch: 6 [52480/60000 (87%)]\tLoss: 106.330589\n",
      "Train Epoch: 6 [53760/60000 (90%)]\tLoss: 109.130264\n",
      "Train Epoch: 6 [55040/60000 (92%)]\tLoss: 110.259956\n",
      "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 111.008530\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 104.594505\n",
      "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 111.558609\n",
      "====> Epoch: 6 Average loss: 108.2999\n",
      "====> Test set loss: 98.3842\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 111.234085\n",
      "Train Epoch: 7 [1280/60000 (2%)]\tLoss: 109.645729\n",
      "Train Epoch: 7 [2560/60000 (4%)]\tLoss: 107.803894\n",
      "Train Epoch: 7 [3840/60000 (6%)]\tLoss: 108.870399\n",
      "Train Epoch: 7 [5120/60000 (9%)]\tLoss: 107.923553\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 108.448341\n",
      "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 104.863533\n",
      "Train Epoch: 7 [8960/60000 (15%)]\tLoss: 106.777374\n",
      "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 108.947197\n",
      "Train Epoch: 7 [11520/60000 (19%)]\tLoss: 112.405479\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 110.614021\n",
      "Train Epoch: 7 [14080/60000 (23%)]\tLoss: 106.361160\n",
      "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 106.756622\n",
      "Train Epoch: 7 [16640/60000 (28%)]\tLoss: 106.793556\n",
      "Train Epoch: 7 [17920/60000 (30%)]\tLoss: 113.084869\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 106.440605\n",
      "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 108.464119\n",
      "Train Epoch: 7 [21760/60000 (36%)]\tLoss: 109.814468\n",
      "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 109.505699\n",
      "Train Epoch: 7 [24320/60000 (41%)]\tLoss: 109.505920\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 110.979973\n",
      "Train Epoch: 7 [26880/60000 (45%)]\tLoss: 102.548874\n",
      "Train Epoch: 7 [28160/60000 (47%)]\tLoss: 107.455551\n",
      "Train Epoch: 7 [29440/60000 (49%)]\tLoss: 110.034668\n",
      "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 108.194191\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 104.947945\n",
      "Train Epoch: 7 [33280/60000 (55%)]\tLoss: 107.903053\n",
      "Train Epoch: 7 [34560/60000 (58%)]\tLoss: 104.816757\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 7 [35840/60000 (60%)]\tLoss: 108.715820\n",
      "Train Epoch: 7 [37120/60000 (62%)]\tLoss: 109.702118\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 107.469727\n",
      "Train Epoch: 7 [39680/60000 (66%)]\tLoss: 107.641518\n",
      "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 104.378700\n",
      "Train Epoch: 7 [42240/60000 (70%)]\tLoss: 104.906540\n",
      "Train Epoch: 7 [43520/60000 (72%)]\tLoss: 107.486771\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 108.889473\n",
      "Train Epoch: 7 [46080/60000 (77%)]\tLoss: 104.638390\n",
      "Train Epoch: 7 [47360/60000 (79%)]\tLoss: 107.324936\n",
      "Train Epoch: 7 [48640/60000 (81%)]\tLoss: 105.836311\n",
      "Train Epoch: 7 [49920/60000 (83%)]\tLoss: 106.953873\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 109.550835\n",
      "Train Epoch: 7 [52480/60000 (87%)]\tLoss: 107.109138\n",
      "Train Epoch: 7 [53760/60000 (90%)]\tLoss: 108.635193\n",
      "Train Epoch: 7 [55040/60000 (92%)]\tLoss: 106.635330\n",
      "Train Epoch: 7 [56320/60000 (94%)]\tLoss: 108.545006\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 103.551331\n",
      "Train Epoch: 7 [58880/60000 (98%)]\tLoss: 104.508865\n",
      "====> Epoch: 7 Average loss: 107.4453\n",
      "====> Test set loss: 97.5619\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 110.414192\n",
      "Train Epoch: 8 [1280/60000 (2%)]\tLoss: 109.017769\n",
      "Train Epoch: 8 [2560/60000 (4%)]\tLoss: 105.340256\n",
      "Train Epoch: 8 [3840/60000 (6%)]\tLoss: 107.913643\n",
      "Train Epoch: 8 [5120/60000 (9%)]\tLoss: 107.704742\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 110.725403\n",
      "Train Epoch: 8 [7680/60000 (13%)]\tLoss: 107.288002\n",
      "Train Epoch: 8 [8960/60000 (15%)]\tLoss: 105.150887\n",
      "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 109.905930\n",
      "Train Epoch: 8 [11520/60000 (19%)]\tLoss: 109.722198\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 104.959976\n",
      "Train Epoch: 8 [14080/60000 (23%)]\tLoss: 105.396492\n",
      "Train Epoch: 8 [15360/60000 (26%)]\tLoss: 105.030159\n",
      "Train Epoch: 8 [16640/60000 (28%)]\tLoss: 107.662498\n",
      "Train Epoch: 8 [17920/60000 (30%)]\tLoss: 103.621674\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 107.617004\n",
      "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 107.714226\n",
      "Train Epoch: 8 [21760/60000 (36%)]\tLoss: 105.612373\n",
      "Train Epoch: 8 [23040/60000 (38%)]\tLoss: 107.255867\n",
      "Train Epoch: 8 [24320/60000 (41%)]\tLoss: 104.859100\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 111.588638\n",
      "Train Epoch: 8 [26880/60000 (45%)]\tLoss: 107.772171\n",
      "Train Epoch: 8 [28160/60000 (47%)]\tLoss: 105.127571\n",
      "Train Epoch: 8 [29440/60000 (49%)]\tLoss: 107.776764\n",
      "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 106.576569\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 106.966621\n",
      "Train Epoch: 8 [33280/60000 (55%)]\tLoss: 108.384659\n",
      "Train Epoch: 8 [34560/60000 (58%)]\tLoss: 109.053078\n",
      "Train Epoch: 8 [35840/60000 (60%)]\tLoss: 106.421143\n",
      "Train Epoch: 8 [37120/60000 (62%)]\tLoss: 104.947662\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 105.631287\n",
      "Train Epoch: 8 [39680/60000 (66%)]\tLoss: 105.336128\n",
      "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 107.615746\n",
      "Train Epoch: 8 [42240/60000 (70%)]\tLoss: 107.420784\n",
      "Train Epoch: 8 [43520/60000 (72%)]\tLoss: 106.509720\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 101.597977\n",
      "Train Epoch: 8 [46080/60000 (77%)]\tLoss: 110.059372\n",
      "Train Epoch: 8 [47360/60000 (79%)]\tLoss: 110.744102\n",
      "Train Epoch: 8 [48640/60000 (81%)]\tLoss: 109.303047\n",
      "Train Epoch: 8 [49920/60000 (83%)]\tLoss: 107.137894\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 112.139282\n",
      "Train Epoch: 8 [52480/60000 (87%)]\tLoss: 105.281860\n",
      "Train Epoch: 8 [53760/60000 (90%)]\tLoss: 106.344261\n",
      "Train Epoch: 8 [55040/60000 (92%)]\tLoss: 104.998192\n",
      "Train Epoch: 8 [56320/60000 (94%)]\tLoss: 108.412086\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 107.797234\n",
      "Train Epoch: 8 [58880/60000 (98%)]\tLoss: 110.470116\n",
      "====> Epoch: 8 Average loss: 106.8290\n",
      "====> Test set loss: 98.1090\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 106.648453\n",
      "Train Epoch: 9 [1280/60000 (2%)]\tLoss: 104.449379\n",
      "Train Epoch: 9 [2560/60000 (4%)]\tLoss: 108.108452\n",
      "Train Epoch: 9 [3840/60000 (6%)]\tLoss: 106.079231\n",
      "Train Epoch: 9 [5120/60000 (9%)]\tLoss: 105.144630\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 108.861740\n",
      "Train Epoch: 9 [7680/60000 (13%)]\tLoss: 107.624985\n",
      "Train Epoch: 9 [8960/60000 (15%)]\tLoss: 104.258881\n",
      "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 105.526344\n",
      "Train Epoch: 9 [11520/60000 (19%)]\tLoss: 108.879532\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 104.964645\n",
      "Train Epoch: 9 [14080/60000 (23%)]\tLoss: 104.207932\n",
      "Train Epoch: 9 [15360/60000 (26%)]\tLoss: 109.355591\n",
      "Train Epoch: 9 [16640/60000 (28%)]\tLoss: 104.600563\n",
      "Train Epoch: 9 [17920/60000 (30%)]\tLoss: 106.429253\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 106.148727\n",
      "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 105.445839\n",
      "Train Epoch: 9 [21760/60000 (36%)]\tLoss: 105.432083\n",
      "Train Epoch: 9 [23040/60000 (38%)]\tLoss: 106.713379\n",
      "Train Epoch: 9 [24320/60000 (41%)]\tLoss: 104.097519\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 108.107674\n",
      "Train Epoch: 9 [26880/60000 (45%)]\tLoss: 109.729469\n",
      "Train Epoch: 9 [28160/60000 (47%)]\tLoss: 108.137405\n",
      "Train Epoch: 9 [29440/60000 (49%)]\tLoss: 106.421585\n",
      "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 106.509003\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 105.732376\n",
      "Train Epoch: 9 [33280/60000 (55%)]\tLoss: 107.233894\n",
      "Train Epoch: 9 [34560/60000 (58%)]\tLoss: 110.121956\n",
      "Train Epoch: 9 [35840/60000 (60%)]\tLoss: 106.744865\n",
      "Train Epoch: 9 [37120/60000 (62%)]\tLoss: 105.190834\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 109.299034\n",
      "Train Epoch: 9 [39680/60000 (66%)]\tLoss: 108.400818\n",
      "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 102.096100\n",
      "Train Epoch: 9 [42240/60000 (70%)]\tLoss: 106.756027\n",
      "Train Epoch: 9 [43520/60000 (72%)]\tLoss: 107.420204\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 105.835472\n",
      "Train Epoch: 9 [46080/60000 (77%)]\tLoss: 104.142197\n",
      "Train Epoch: 9 [47360/60000 (79%)]\tLoss: 101.526459\n",
      "Train Epoch: 9 [48640/60000 (81%)]\tLoss: 106.998741\n",
      "Train Epoch: 9 [49920/60000 (83%)]\tLoss: 105.266342\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 102.530899\n",
      "Train Epoch: 9 [52480/60000 (87%)]\tLoss: 102.953415\n",
      "Train Epoch: 9 [53760/60000 (90%)]\tLoss: 101.930054\n",
      "Train Epoch: 9 [55040/60000 (92%)]\tLoss: 105.374046\n",
      "Train Epoch: 9 [56320/60000 (94%)]\tLoss: 101.348953\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 105.379173\n",
      "Train Epoch: 9 [58880/60000 (98%)]\tLoss: 104.352119\n",
      "====> Epoch: 9 Average loss: 106.3513\n",
      "====> Test set loss: 96.7523\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 103.665543\n",
      "Train Epoch: 10 [1280/60000 (2%)]\tLoss: 103.602615\n",
      "Train Epoch: 10 [2560/60000 (4%)]\tLoss: 109.002701\n",
      "Train Epoch: 10 [3840/60000 (6%)]\tLoss: 105.714920\n",
      "Train Epoch: 10 [5120/60000 (9%)]\tLoss: 109.035141\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 106.674797\n",
      "Train Epoch: 10 [7680/60000 (13%)]\tLoss: 107.612160\n",
      "Train Epoch: 10 [8960/60000 (15%)]\tLoss: 104.867867\n",
      "Train Epoch: 10 [10240/60000 (17%)]\tLoss: 107.426697\n",
      "Train Epoch: 10 [11520/60000 (19%)]\tLoss: 105.202171\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 102.419968\n",
      "Train Epoch: 10 [14080/60000 (23%)]\tLoss: 111.554077\n",
      "Train Epoch: 10 [15360/60000 (26%)]\tLoss: 108.228195\n",
      "Train Epoch: 10 [16640/60000 (28%)]\tLoss: 105.875702\n",
      "Train Epoch: 10 [17920/60000 (30%)]\tLoss: 107.289467\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 107.548538\n",
      "Train Epoch: 10 [20480/60000 (34%)]\tLoss: 104.202423\n",
      "Train Epoch: 10 [21760/60000 (36%)]\tLoss: 104.841827\n",
      "Train Epoch: 10 [23040/60000 (38%)]\tLoss: 106.357841\n",
      "Train Epoch: 10 [24320/60000 (41%)]\tLoss: 106.663055\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 109.099991\n",
      "Train Epoch: 10 [26880/60000 (45%)]\tLoss: 104.579300\n",
      "Train Epoch: 10 [28160/60000 (47%)]\tLoss: 105.702133\n",
      "Train Epoch: 10 [29440/60000 (49%)]\tLoss: 103.988861\n",
      "Train Epoch: 10 [30720/60000 (51%)]\tLoss: 106.679054\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 103.409912\n",
      "Train Epoch: 10 [33280/60000 (55%)]\tLoss: 105.130058\n",
      "Train Epoch: 10 [34560/60000 (58%)]\tLoss: 103.801407\n",
      "Train Epoch: 10 [35840/60000 (60%)]\tLoss: 107.058029\n",
      "Train Epoch: 10 [37120/60000 (62%)]\tLoss: 106.809097\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 104.594208\n",
      "Train Epoch: 10 [39680/60000 (66%)]\tLoss: 109.211128\n",
      "Train Epoch: 10 [40960/60000 (68%)]\tLoss: 104.726257\n",
      "Train Epoch: 10 [42240/60000 (70%)]\tLoss: 104.049118\n",
      "Train Epoch: 10 [43520/60000 (72%)]\tLoss: 103.835983\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 103.602570\n",
      "Train Epoch: 10 [46080/60000 (77%)]\tLoss: 105.071823\n",
      "Train Epoch: 10 [47360/60000 (79%)]\tLoss: 106.644318\n",
      "Train Epoch: 10 [48640/60000 (81%)]\tLoss: 105.572083\n",
      "Train Epoch: 10 [49920/60000 (83%)]\tLoss: 104.267029\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 103.102295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 10 [52480/60000 (87%)]\tLoss: 104.774734\n",
      "Train Epoch: 10 [53760/60000 (90%)]\tLoss: 107.218857\n",
      "Train Epoch: 10 [55040/60000 (92%)]\tLoss: 100.997055\n",
      "Train Epoch: 10 [56320/60000 (94%)]\tLoss: 109.691734\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 106.882431\n",
      "Train Epoch: 10 [58880/60000 (98%)]\tLoss: 105.880547\n",
      "====> Epoch: 10 Average loss: 105.8832\n",
      "====> Test set loss: 97.0524\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "epochs=10\n",
    "seed=1\n",
    "cuda = torch.cuda.is_available()\n",
    "log_interval = 10\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.ToTensor()),\n",
    "    batch_size=batch_size, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.ToTensor()),\n",
    "    batch_size=batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(784, 400)\n",
    "        self.fc21 = nn.Linear(400, 20)\n",
    "        self.fc22 = nn.Linear(400, 20)\n",
    "        self.fc3 = nn.Linear(20, 400)\n",
    "        self.fc4 = nn.Linear(400, 784)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        if self.training:\n",
    "            std = torch.exp(0.5*logvar)\n",
    "            eps = torch.randn_like(std)\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        return F.sigmoid(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, 784))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "\n",
    "model = VAE().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), size_average=False)\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return BCE + KLD\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                loss.item() / len(data)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(train_loader.dataset)))\n",
    "\n",
    "\n",
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (data, _) in enumerate(test_loader):\n",
    "            data = data.to(device)\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            test_loss += loss_function(recon_batch, data, mu, logvar).item()\n",
    "            if i == 0:\n",
    "                n = min(data.size(0), 8)\n",
    "                comparison = torch.cat([data[:n],\n",
    "                                      recon_batch.view(batch_size, 1, 28, 28)[:n]])\n",
    "                save_image(comparison.cpu(),\n",
    "                         'results/reconstruction_' + str(epoch) + '.png', nrow=n)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))\n",
    "\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch)\n",
    "    test(epoch)\n",
    "    with torch.no_grad():\n",
    "        sample = torch.randn(64, 20).to(device)\n",
    "        sample = model.decode(sample).cpu()\n",
    "        save_image(sample.view(64, 1, 28, 28),\n",
    "                   'results/sample_' + str(epoch) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
